1. Supervised learning is when you have a ground truth to train the model. Unsupervised learning is where the model does not have a ground truth, but rather finds patterns in the data. Weakly supervised is where the ground truth is noisy or of less quality. An example would be using a set of rules to determine the labels of something. Both semi-supervised and active learning use a limited amount of labeled data and a larger amount of unlabeled data. The difference is that for semi-supervised, the model is trained on the labeled data, and the high confidence predictions are deemed as labeled, and added to the training data. For active learning, the low confidence predictions are singled out and a human labels them, and they are added to the training data.
2. 
    i. ERM is basically minimizing the loss function in an ML problem. The risk is overfitting, because we're minimizing the training loss, and we're at risk of forgoing the generalization loss. There is also the bias/variance tradeoff, where we need to carefully balance the model's flexibility (low bias) and ability to generalize (low vairance)
    ii. In ERM, the loss is calculated from the training data, which is an emiprical sample from the true, unknown data distribution. 
    iii. Regularization, careful loss function choice
3. Skip
4. Compute
5. Generally, deep. With more non-linear activation layers, the deep NN is able to capture more of the non-linear patterns in the data. Moreover, with the depth, its able to better capture the hierarchical representations of the data. However, this depends on the data.
6. Skip
7. Saddle points is where the gradient is 0, but its neither a local min nor max, and the second order Hessian is indefinite. Think x^3 function. Local minima is a point where the gradient is 0, but it is not the global min, which is a suboptimal solution. Local minima pose a larger problem to deep NNs because it's often harder to 'escape'.
8. 
    i. Skip
    ii. Skip
    iii. Random Search: randomly sample a given set of distributions for hyperparameters. Hyperband: an improvement on random search, where it early stops poorly performing configs.
9. Too EZ
10. 
    i. A parametric model makes strong assumptions about the distrubtion of the underlying data, and requires manual adding of the parameters. An example is linear regression. A non parametric model is simply not that. It makes fewer or no assumptions about the underlying distribution of the data, and does not require manual definition of the parameters. The parameters are instead learned from the data itself. An example is a NN.
    ii. It depends. We use parametric model if the dataset is small, the assumptions of the model are met by the dataset, and we need interpretability. If the dataset is large, we do not know the relationships in the data (or are complex), we use non parametric.
11. Think of it as reducing variance. By independently training models (on a subset of data even), we allow the model to be exposed to different "chunks" in the data, and by averaging these models, we effectively smooth out any variance that this shift in training data might have, and thus lowering variance, and thus improving generalization. This alsmo makes the model more robust to noise, as individual models may be sensitive to the noise, but by averaging, we can smooth it out. Finally, by having multiple models, each model is exploring a different region of the hypothesis space, and thus improves the chance of finding a hypothesis that generalizes well.
12. Think of the derivatives themselves. For L1, we have the L1 norm which is an absolute value. The derivative of that is sign(w_i). This provides a "constant force" when updating the value of the weights regardless of the weights themselves. The derivatove of L2 norm is 2w_i, which is a force proportional to the size of w_i. So it can push it very close to 0, but because its proportional, it wont go to 0.
13. 
    1. Training data: the sampling methods used to sample the training data introduced bias (selection bias). For example, if you were to predict the type of pizza people prefer based on their age, and you only sample from highschoolers, that would be bias.
    2. Model: the model requires assumptions in the data that are not met in prod
    3. Test data: Data drift. This includes concept drift, where the underlying relationship between the input and output has changed (for example, customer preferenece shift), or a covariate drift, where the input data distribution has shifted, but the relationship between input and output remains the same. For example, if the model was trained on users of one demographic, shifting the model to another demographic might cause performace degradation because the demographic might be different. The final one is label drift. The underlying distribution in the labels have shifted, but the distribution of the input given these labels have not. 
14. 
    1. Component efficiency. For complex models with many components, its important to conduct ablation studies, where we remove components of the model to see how the performance changes. We can remove the parts that are slow and/or give minor improvements
    2. Server / client side inference. Inferencing on edge devices consume local memory and battery, and it makes it diffifuclt to collect user feedback due to limited context, lack of data (input / output info), and inconsistent feedback. Server side inference fixes many of these issues (all input an output and context is known, making it easier to construct the full picture of feedback), but introduces higher latency and potential privacy concerns.
    3. Interpretability. Larger can more complex models are better performing, but they lack interpretability. Sometimes we need to know why a prediction is such, an example is why someone's loan application was denied
    4. Bias. We must ensure that the model does not perpetuate any biases in the training data, such as race or gender stereotypes
15. 
    1. The main reasons above: covariate shift and concept shift.
    2. For covariate shift and label shift (P(X) or P(Y) has changed), we can compare the 2 distributions using statistical tests, like the Kolmogorov-Smirnov test. For concept shift, (P(Y|X) has shifted), the process is more involved. We can periodically evaluate the model on a holdout set that reflects the current prod env, or we can train a drift detector on the current production data. A major shift in the predictions from the original model and this one shows a probable concept drift.
    3. The most commonly used approach is to retrain the model using the labeled data from the target distribution. We can retrain fully, or finetune. We would start the finetuning on the data from the time where the degredation began